\section{Method}

\subsection{Source of data}

For this comparative study the following data sources will be used
\begin{enumerate}
	\item \gls{ac:nibio}
	\item Xgeo
	\item \gls{ac:kilden}
	\item \gls{ac:met}
\end{enumerate}

\subsection{Dataset}

The dataset is chosen from four regions in Norway; Innlandet, Vestfold, Trøndelag, and Østfold. From each region are four stations picked:
\begin{multicols}{2}
\begin{itemize}
	\item[Innlandet] \begin{enumerate}
		\item Kise
		\item Ilseng
		\item Apelsvoll
		\item Gausdal
	\end{enumerate}
	\item[Østfold] \begin{enumerate}
		\item Rygge
		\item Rakkestad
		\item Tomb
		\item Øsaker
	\end{enumerate}
	\item[Trøndelag] \begin{enumerate}
		\item Kvithamar
		\item Rissa
		\item Frosta
		\item Mære
	\end{enumerate}
	\item[Vestfold] \begin{enumerate}
		\item Lier
		\item Ramnes
		\item Tjølling
		\item Sande
	\end{enumerate}
\end{itemize}
\end{multicols}

All stations are sampled from the date\footnote{Format month-day} 03-01 to 10-31 from 2016 to 2020. The features rain (RR), mean soil temperature at 10cm (TJM10), mean soil temperature at 20cm (TJM20), and air temperature at 2m (TM) are sampled from the LMT database. The snow parameter is sampled from MET via Xgeo for imputed values in areas where there are no messured values. The soil type, and soil texture is sampled from Kilden from Norwegian Institute of Bioeconomy Research.

\subsubsection{Selection process}
The selection process for finding these station can be compiled into these steps

\begin{enumerate}
	\item Recommendation from Norwegian Institute of Bioeconomy Research
	\item \label{list:na_anal}Compute the missing values in the data
	\item Missing values analyse 
	\item Searching LMT database for alternative station candidates if current data is insufficient
	\item If some station was replaced the repeat step \ref{list:na_anal}
\end{enumerate}

\begin{figure}
	\centering
	\label{fig:plot-17}
	\includegraphics{"../../plots/plot-17"}
	\caption{Visual representation of missing values at station 17 from 2014 to 2020}
\end{figure}

The plots of stations follow a simple representation where the y-axis represent the year and the x-axis represent the index of the data as all tables are taken from the same period. A circle represent a singluar na values, while a band represent a series of 2 or more missing values. The colours represents the features used in this comperative study. This representation of the missing values will indicate sesonal, and systematic removal of data and give an overall indication of how much data is missing. To get further insight into the data a report is generated in parallel to the plots describing precise date and time of all values and which other parameter values is also missing values in the same period. See appendix \ref{apx:code:dataanal} for the full detail of the report generation and appendix \ref{apx:plots} for na-plots of the station chosen for this study.

\subsubsection{Collection of data}

The method used was a powershell\footnote{Version 7.3.11} script that called the respective institutions servers using the "curl" program\footnote{curl 8.4.0 (Windows) libcurl/8.4.0 Schannel WinIDN} to send an http request for the timeseries starting from 2014 to 2020 in the interval 1 of May to 31 of October. Code for data collection can be viewed in appendix \ref{apx:code:datacollect}. The data is stores as an either a csv file or a json file for easy retrieval and manual control of values.

\subsubsection{Labeling of stations between Nibio and MET}

Since Nibio and MET have different names for the same stations one must compile a list that converts Nibio ID to MET ID. This was performed with these requests 
\begin{table}
	\centering
	\begin{tabular}{r|lp{5cm}|}
		FROST & SQL approximate Code\\\hline
		 Stations with rain & \parbox{8cm}{\lstinline[language=sql]|SELECT StationName FROM FROST WHERE  LIMIT 4|} \\
		 Station ID & \parbox{8cm}{\lstinline[language=sql]|SELECT StationID, LMTID FROM FROST,LMT WHERE |} \\
		\hline LMT & Code \\\hline
		Meteorological data & \parbox{8cm}{\lstinline[language=sql]|SELECT ID,date,TM,RR,TJM10,TJM20 FROM LMT WHERE date IN BETWEEN year-03-01 year-10-31 AND ID = LMTID|} \\
	\end{tabular}
	\caption{SQL version of the query requests sent to the different institutions.}
\end{table}
Where ID is the Nibio Id for the given station, Frost.ID is the MET id, ID.latitude is the latitude gathered from Nibio, ID.longitude is the longitude gathered from Nibio. These variables can be swaped out for the relevant station.

\subsubsection{Storage of data}
The storage of the data is done through two data structures; \gls{gl:hashmap} and \gls{gl:dataframe} from the package pandas. The transformation of data is done with a costume datatype called "DataFileHandler" which is converted to a module for convenience. The keys for the hashmap is chosen by the naming of the data files and the pattern given to the class. To escalete modeling the data will also be exported to a binary file for faster retrieval. 

\begin{figure}[ht]
	\begin{tikzpicture}
		\node (start) [startstop] {Web request};
		\node (storage) [process, right of=start] {file storage};
		\node (reading) [process, right of=storage] {DataFileHandler};
		\node (datatreat) [process, right of=reading] {Data treatment};
		\node (p1) [process, right of=reading] {preprosessing 1};
		\node (p2) at (6,6) {preprosessing 2};
		\node (p3) at (6,4) {preprosessing 3};
		\node (pn) at (6,2) {preprosessing n};
		\node (m1) at (4,8) {modeling 1};
		\node (m2) at (4,6) {modeling 2};
		\node (m3) at (4,4) {modeling 3};
		\node (mn) at (4,2) {modeling n};
	\end{tikzpicture} % fill inn the lines.
	\caption{Compressed structure of study}
\end{figure}

%\paragraph[Data structure]{Technical overview of custom data structure}
The data structure used to store the data from the different stations is called "DataFileHandler" and stores the data in a tree-structure where indexes are dictated by the filename. It has several built-in functions to assist with data partitioning, and merging of data. This makes it easier to move and store all 846 720 observations from 16 station from 4 regions\footnote{there are 4 stations per region.}. 

\subsection{Data cleaning and treatment}

To use the data in this study it must be cleaned and treated for training. The following methods were picked common practice in litterateur with new methods based on the decomposition of the data in the from of \acrfull{ac:stl}\cite{cleveland_stl_1990}\footnote{In this study we expand this for multiple seasons using \acrfull{ac:mstl}\cite{bandara_mstl_2021}, but the theory of this imputation method remains the same.}.

\subsubsection{Outlier detection and removal}

Though the data fetched from \acrshort{ac:nibio} is treated and controlled the external data from \acrshort{ac:met} might not be, and this research project incorporated raw, untreated data from \acrshort{ac:nibio} to fill inn missing values. This paper has done empirical studies to find out which method to use in the prepossessing step of training the models. The selected methods are

\begin{enumerate}
	\item model based
	\begin{itemize}
		\item \acrfull{ac:arima}
		\item LSTM
	\end{itemize}
	\item statistic based
	\begin{itemize}
		\item backwards and forwards first observations
		\item rolling mean
		\item linear imputation
	\end{itemize}
	\item \acrshort{ac:stl} decomposition with above methods
\end{enumerate}

\subsubsection{Missing value imputation}

The data has missing values, in particular during early Fall when there were sub-zero temperatures meaning any rain measurements done during this period would have unpredictable fluctuations since at negative temperatures water can freeze, get clogged up with residual bio-material from the surrounding area

\begin{enumerate}
	\item linear imputation
	\item backwards and forwards first available observation
	\item global mean replacement
	\item STL decomposition with above methods to impute components
\end{enumerate}

The last method, using STL, was chosen because it would in principle be simpler to impute a less noisy signal than a noisy one.

\subsection{Setup of models}

The models are set up in according to the relevant paper the model is fetched from, alternatively reuse the code made by the author. When importing the data to the model there will be modifying to the original code to facilitate for the model as far as it goes. Any modifications will be in the appendix under section \ref{apx:code}. For the convenience of the reader all code is using the sklearn estimator class to make all the models discuses in this study more user friendly and compatible with sklearns other functions. The details of the models will be discussed in section \ref{sec:theory}, this section discusses the setup and implementation of the models.\footnote{Caution to the reader; The code used was run on a Linux subsystem on windows due to the fact that the current version of tensorflow can't run on Windows.}

\subsubsection{Basic Linear model}

The linear model (sec \ref{sec:theory:linreg}) utilises in the study is created from the python model sklearn (or scikit-learn according to pythons package manager) 

\subsection[Use of AI]{Use of Artificial Intelligence in this paper}

In this paper there has been used Artificial Intelligence (AI), specifically Bing Chat / Copilot hosted by Microsoft Cooperation with special agreement with The Norwegian University of Life Sciences, for the following purposes:

\begin{enumerate}
	\item Formalising sentences and rephrasing sentences.
	\item Spellchecking
	\item Code generation of basic consepts and structures (tree traversal, template generic class) 
	\item Better understanding of domain
\end{enumerate}

All code have been manually check and verified in a separate environment and dedicated class for testing and verification. No confidential information or data has been past into the AI and only generic questions regarding broad topics has been prompted to the AI. Any topics discussed with the chat bot / AI were double checked with research papers and textbooks for verification, and any sources brought up by the AI was checked and verified.