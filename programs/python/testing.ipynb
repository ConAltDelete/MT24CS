{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985f3040-ce38-49e9-901e-988c4a1d4179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 21:40:33.393316: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-17 21:40:34.101082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# import relevant modules\n",
    "from My_tools import StudyEstimators as SE\n",
    "from My_tools import DataFileLoader as DFL\n",
    "from ILSTM_Soil_model_main import lstm_interprety_soil_moisture as ILSTM\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "import torch\n",
    "\n",
    "ROOT = \"../../\"\n",
    "\n",
    "RESULT_PATH = ROOT + \"results/\"\n",
    "\n",
    "DATA_PATH = ROOT + \"data/\"\n",
    "PLOT_PATH = RESULT_PATH + \"plots/\"\n",
    "TABLE_PATH = RESULT_PATH + \"tables/\"\n",
    "OTHER_PATH = RESULT_PATH + \"other/\"\n",
    "\n",
    "METADATA_PRELOAD_DATA_PATH = OTHER_PATH + \"bin_data/\"\n",
    "\n",
    "DATA_INFO = DATA_PATH + \"info/\"\n",
    "DATA_INFO_NIBIO_FILE = DATA_INFO  + \"lmt.nibio.csv\"\n",
    "DATA_INFO_FROST_FILE = DATA_INFO + \"Frost_stations.csv\"\n",
    "DATA_INFO_NIBIO2FROST_FILE = DATA_INFO + \"StationIDInfo.csv\"\n",
    "DATA_FILE_SOIL_STATIONS = DATA_INFO + \"'Stasjonsliste jordtemperatur modellering.xlsx'\"\n",
    "\n",
    "DATA_COLLECTION = DATA_PATH + \"raw_data/\"\n",
    "DATA_COLLECTION_STAT = DATA_COLLECTION + \"Veret paa Aas 2013- 2017/\" # pattern -> 'Veret paa Aas 2013- 2017/Veret paa Aas {YYYY}.pdf'\n",
    "DATA_COLLECTION_TIME = DATA_COLLECTION + \"Time 2013- 2023/\" # pattern -> Time{YYYY}.xlsx\n",
    "DATA_COLLECTION_NIBIO = DATA_COLLECTION + \"nibio/\" # pattern -> weather_data_hour_stID{id}_y{year}.csv\n",
    "DATA_COLLECTION_MET = DATA_COLLECTION + \"MET/\" # pattern -> StationTo_{id}_FROM_{FrostID}.csv\n",
    "\n",
    "# ID definitions\n",
    "station_names = pd.read_csv(DATA_INFO_NIBIO_FILE,\n",
    "                          header=0,\n",
    "                          index_col = \"ID\")\n",
    "\n",
    "nibio_id = {\n",
    "    \"Innlandet\" : [\"11\",\"17\",\"26\",\"27\"],\n",
    "    \"Trøndelag\" : [\"15\",\"57\",\"34\",\"39\"],\n",
    "    \"Østfold\" : [\"37\",\"41\",\"52\",\"118\"],\n",
    "    \"Vestfold\" : [\"30\",\"38\",\"42\",\"50\"] # Fjern \"50\" for å se om bedre resultat\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4d8e5d-fd09-419b-9366-5be457a701ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_load = False\n",
    "if force_load:\n",
    "    nibio_data_ungroup = DFL.DataFileLoader(DATA_COLLECTION_NIBIO,r\"weather_data_hour_stID(\\d{1,3})_y(\\d{4}).csv\",_iter_key = True)\n",
    "    nibio_data_ungroup.load_data(names = [\"Time\",\"TM\",\"RR\",\"TJM10\",\"TJM20\"])\n",
    "    nibio_data = nibio_data_ungroup.group_layer(nibio_id)\n",
    "\n",
    "    nibio_data_raw_ungroup = DFL.DataFileLoader(DATA_COLLECTION_NIBIO,r\"weather_data_raw_hour_stID(\\d{1,3})_y(\\d{4}).csv\",_iter_key = True)\n",
    "    nibio_data_raw_ungroup.load_data(names = [\"Time\",\"TM\",\"RR\",\"TJM10\",\"TJM20\"])\n",
    "    nibio_data_raw = nibio_data_raw_ungroup.group_layer(nibio_id)\n",
    "\n",
    "    frost_raw_ungroup = DFL.DataFileLoader(DATA_COLLECTION_MET,r\"weather_data_raw_hour_stID(\\d{1,3})_y(\\d{4}).csv\",_iter_key = True)\n",
    "\n",
    "    def dataframe_merge_func(x,y):\n",
    "        y.iloc[y.iloc[:,1].notna() & (y.iloc[:,1] <= 0),2] = pd.NA\n",
    "        x.iloc[0:y.shape[0],2] = y.iloc[0:y.shape[0],2]\n",
    "        return x\n",
    "\n",
    "    imputed_nibio_data = nibio_data.combine(nibio_data_raw,merge_func = dataframe_merge_func)\n",
    "    imputed_nibio_data.dump(METADATA_PRELOAD_DATA_PATH + \"weatherdata.bin\")\n",
    "\n",
    "    del nibio_data, nibio_data_raw, frost_raw_ungroup, nibio_data_raw_ungroup, nibio_data_ungroup\n",
    "else: \n",
    "    imputed_nibio_data = DFL.DataFileLoader().load(METADATA_PRELOAD_DATA_PATH + \"weatherdata_cleaned.bin\")\n",
    "\n",
    "terskel_data = pd.read_csv(TABLE_PATH + \"na_run_count_simp.csv\",delimiter=\";\")\n",
    "terskel = int(next(t.split(\">\")[-1] for t in terskel_data.columns if \">\" in t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a00de72-032f-44f3-a1fd-865571fa5363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matsho/.local/lib/python3.9/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/matsho/.local/lib/python3.9/site-packages/torch/nn/modules/loss.py:933: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
      "/home/matsho/.local/lib/python3.9/site-packages/torch/nn/modules/loss.py:933: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n",
      "/home/matsho/.local/lib/python3.9/site-packages/torch/nn/modules/loss.py:933: UserWarning: Using a target size (torch.Size([840])) that is different to the input size (torch.Size([840, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train_Loss: 0.02878357656300068 validation_Loss: 0.04045752435922623\n",
      "1 train_Loss: 0.02207169122993946 validation_Loss: 0.02919645980000496\n",
      "2 train_Loss: 0.0002678249729797244 validation_Loss: 0.0011171418009325862\n",
      "3 train_Loss: 0.000882941298186779 validation_Loss: 0.0009373935754410923\n",
      "4 train_Loss: 0.00018685297982301563 validation_Loss: 0.0012034772662445903\n",
      "5 train_Loss: 0.00032090445165522397 validation_Loss: 0.0010883298236876726\n",
      "6 train_Loss: 0.0008465780410915613 validation_Loss: 0.0011330123525112867\n",
      "7 train_Loss: 0.0004964119871146977 validation_Loss: 0.0011562618892639875\n",
      "8 train_Loss: 0.00011478031956357881 validation_Loss: 0.001368611236102879\n",
      "9 train_Loss: 5.4972821089904755e-05 validation_Loss: 0.001618275186046958\n",
      "[[1.9200133]\n",
      " [1.9323187]\n",
      " [1.9343201]\n",
      " ...\n",
      " [5.895031 ]\n",
      " [5.822282 ]\n",
      " [5.754538 ]]\n",
      "[1.9200133 1.9323187 1.9343201 ... 5.895031  5.822282  5.754538 ] 5803\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def ILSTM_train(raw_data, target_label,total_epoch = 50,hidden_size=16,lerningrate=1e-3, lead_time=1, seq_length=24, batch_size=16):\n",
    "    data,scaler,scaler1 = ILSTM.nibio_data_transform(raw_data, target_label)\n",
    "    data = scaler1.transform(data)\n",
    "\n",
    "    # TODO: Generate the tensor for lstm model\n",
    "\n",
    "    [data_x, data_y,data_z] = ILSTM.LSTMDataGenerator(data, lead_time, batch_size, seq_length)\n",
    "\n",
    "       # concat all variables.\n",
    "    # TODO: Flexible valid split\n",
    "    data_train_x=data_x[:int((data_x.shape[0])-400*24)]\n",
    "    data_train_y = data_y[:int(data_x.shape[0]-400*24)]\n",
    "\n",
    "    train_data = Data.TensorDataset(data_train_x, data_train_y)\n",
    "    train_loader = Data.DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    data_valid_x=data_x[int(data_x.shape[0]-400*24):int(data_x.shape[0]-365*24)] # -> trener 35 dager\n",
    "    data_valid_y=data_y[int(data_x.shape[0]-400*24):int(data_x.shape[0]-365*24)] # -> tester 35 dager\n",
    "    #data_test_x=data_x[int(data_x.shape[0]):int(1.0 * data_x.shape[0])] # -> validerer på resterende\n",
    "    #data_testd_z=data_z[int(data_x.shape[0]-365*24):int(1.0 * data_x.shape[0])] # -> stat på rest\n",
    "\n",
    "    # TODO: Flexible input shapes and optimizer\n",
    "    # IMVTensorLSTM,IMVFullLSTM\n",
    "    model = ILSTM.ILSTM_SV(data_x.shape[2],data_x.shape[1], 1, hidden_size)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    # TODO: Trian LSTM based on the training and validation sets\n",
    "    model,predicor_import,temporal_import=ILSTM.train_lstm(model,lerningrate,total_epoch,train_loader,data_valid_x,data_valid_y,\"./saved_models/lstm_1d.h5\")\n",
    "\n",
    "    # TODO: Create predictions based on the test sets\n",
    "    pred, mulit_FV_aten, predicor_import,temporal_import = ILSTM.create_predictions(model, data_x,scaler)\n",
    "    # TODO: Computer score of R2 and RMSE\n",
    "    print(pred)\n",
    "\n",
    "    return pred.flatten()\n",
    "    \n",
    "\n",
    "\n",
    "# Need to transform the data first to fit the model.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def datetime2string(x):\n",
    "    x[\"Time\"] = x[\"Time\"].apply(lambda y: y.strftime(\"%Y-%m-%d %X\"))\n",
    "    return x\n",
    "station_data = imputed_nibio_data.data_transform(datetime2string).shave_top_layer()[\"11\"].flatten()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "pred = ILSTM_train(copy.deepcopy(station_data[0]),\"TJM20\",batch_size = 8,total_epoch = 10)\n",
    "print(pred, len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac91c35-5d42-4915-b813-9a00609e7afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5880"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(station_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec95f2f6-3506-47f7-8f22-9e985677d2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
