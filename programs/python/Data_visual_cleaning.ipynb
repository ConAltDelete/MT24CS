{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f8bb7f-4b1b-455f-981a-348af3f41f67",
   "metadata": {},
   "source": [
    "# Data visualisering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db14a892-39f8-4cdb-b306-2833714a14ac",
   "metadata": {},
   "source": [
    "## Loading relevant modules\n",
    "\n",
    "Loading relevant modules and creating relevant (and relative) path. Feel free to change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28bd3779-78fc-4d72-ab06-c2052a580345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from My_tools import DataFileLoader as DFL\n",
    "# from My_tools import StudyEstimators as SE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "\n",
    "# imputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer,IterativeImputer,KNNImputer\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# visualising\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# modeling\n",
    "import statsmodels as sm \n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91bf8c59-107a-4190-ad7d-099b0d83cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"../../\"\n",
    "\n",
    "RESULT_PATH = ROOT + \"results/\"\n",
    "\n",
    "DATA_PATH = ROOT + \"data/\"\n",
    "PLOT_PATH = RESULT_PATH + \"plots/\"\n",
    "TABLE_PATH = RESULT_PATH + \"tables/\"\n",
    "OTHER_PATH = RESULT_PATH + \"other/\"\n",
    "AUGME_PATH = RESULT_PATH + \"data/\"\n",
    "\n",
    "METADATA_PRELOAD_DATA_PATH = OTHER_PATH + \"bin_data/\"\n",
    "\n",
    "DATA_INFO = DATA_PATH + \"info/\"\n",
    "DATA_INFO_NIBIO_FILE = DATA_INFO  + \"lmt.nibio.csv\"\n",
    "DATA_INFO_FROST_FILE = DATA_INFO + \"Frost_stations.csv\"\n",
    "DATA_INFO_NIBIO2FROST_FILE = DATA_INFO + \"StationIDInfo.csv\"\n",
    "DATA_FILE_SOIL_STATIONS = DATA_INFO + \"'Stasjonsliste jordtemperatur modellering.xlsx'\"\n",
    "\n",
    "DATA_COLLECTION = DATA_PATH + \"raw_data/\"\n",
    "DATA_COLLECTION_STAT = DATA_COLLECTION + \"Veret paa Aas 2013- 2017/\" # pattern -> 'Veret paa Aas 2013- 2017/Veret paa Aas {YYYY}.pdf'\n",
    "DATA_COLLECTION_TIME = DATA_COLLECTION + \"Time 2013- 2023/\" # pattern -> Time{YYYY}.xlsx\n",
    "DATA_COLLECTION_NIBIO = DATA_COLLECTION + \"nibio/\" # pattern -> weather_data_hour_stID{id}_y{year}.csv\n",
    "DATA_COLLECTION_MET = DATA_COLLECTION + \"MET/\" # pattern -> StationTo_{id}_FROM_{FrostID}.csv\n",
    "\n",
    "# ID definitions\n",
    "station_names = pd.read_csv(DATA_INFO_NIBIO_FILE,\n",
    "                          header=0,\n",
    "                          index_col = \"ID\")\n",
    "\n",
    "nibio_id = {\n",
    "    \"Innlandet\" : [\"11\",\"17\",\"26\",\"27\"],\n",
    "    \"Trøndelag\" : [\"15\",\"57\",\"34\",\"39\"],\n",
    "    \"Østfold\" : [\"37\",\"41\",\"52\",\"118\"],\n",
    "    \"Vestfold\" : [\"30\",\"38\",\"42\",\"50\"] # Fjern \"50\" for å se om bedre resultat\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ddfc80-3652-4c70-a086-aa91cd5a6940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "287b354f-4989-4a46-b1ab-e8d4452a9f01",
   "metadata": {},
   "source": [
    "## Fetching and ploting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8092c3-583c-4311-ab3a-0dc71b57e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_load = False\n",
    "if force_load:\n",
    "    nibio_data_ungroup = DFL.DataFileLoader(DATA_COLLECTION_NIBIO,r\"weather_data_hour_stID(\\d{1,3})_y(\\d{4}).csv\",_iter_key = True)\n",
    "    nibio_data_ungroup.load_data(names = [\"Time\",\"TM\",\"RR\",\"TJM10\",\"TJM20\"])\n",
    "    nibio_data = nibio_data_ungroup.group_layer(nibio_id)\n",
    "\n",
    "    nibio_data_raw_ungroup = DFL.DataFileLoader(DATA_COLLECTION_NIBIO,r\"weather_data_raw_hour_stID(\\d{1,3})_y(\\d{4}).csv\",_iter_key = True)\n",
    "    nibio_data_raw_ungroup.load_data(names = [\"Time\",\"TM\",\"RR\",\"TJM10\",\"TJM20\"])\n",
    "    nibio_data_raw = nibio_data_raw_ungroup.group_layer(nibio_id)\n",
    "\n",
    "    frost_raw_ungroup = DFL.DataFileLoader(DATA_COLLECTION_MET,r\"weather_data_raw_hour_stID(\\d{1,3})_y(\\d{4}).csv\",_iter_key = True)\n",
    "\n",
    "    def dataframe_merge_func(x,y):\n",
    "        y.iloc[y.iloc[:,1].notna() & (y.iloc[:,1] <= 0),2] = pd.NA\n",
    "        x.iloc[0:y.shape[0],2] = y.iloc[0:y.shape[0],2]\n",
    "        return x\n",
    "\n",
    "    imputed_nibio_data = nibio_data.combine(nibio_data_raw,merge_func = dataframe_merge_func)\n",
    "    imputed_nibio_data.dump(METADATA_PRELOAD_DATA_PATH + \"weatherdata.bin\")\n",
    "\n",
    "    del nibio_data, nibio_data_raw, frost_raw_ungroup, nibio_data_raw_ungroup, nibio_data_ungroup\n",
    "else: \n",
    "    imputed_nibio_data = DFL.DataFileLoader().load(METADATA_PRELOAD_DATA_PATH + \"weatherdata.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73ac642-9ca5-4b02-84be-92c6bdbb0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "terskel_data = pd.read_csv(TABLE_PATH + \"na_run_count_simp.csv\",delimiter=\";\")\n",
    "terskel = int(next(t.split(\">\")[-1] for t in terskel_data.columns if \">\" in t))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44f20a4e-4165-4717-acef-9112e96d280e",
   "metadata": {},
   "source": [
    "snow_data = DFL.DataFileLoader(DATA_COLLECTION_MET,r\"weatherdata_snow_stID(\\d{1,3}).csv\",_iter_key = True).load_data(date_format = \"%Y-&m-%dT%H:00:00.000Z\") \n",
    "\n",
    "ref_indexes = imputed_nibio_data[\"Time\"]\n",
    "\n",
    "def extend_data(df):\n",
    "    \"\"\"\n",
    "        df inn, df out\n",
    "        df out is larger than df in.\n",
    "    \"\"\"\n",
    "    # Merge the original dates with the complete range\n",
    "    df_merged = pd.merge(ref_indexes, df, on='dates', how='left', indicator=True)\n",
    "\n",
    "    # Create a boolean column to indicate original dates\n",
    "    df_merged['is_original'] = df[df_merged['_merge'] == 'both']\n",
    "\n",
    "    df_to_append = pd.DataFrame({'values': df[df_merged['_merge'] == 'both'] * extra_rows})\n",
    "\n",
    "    # Slice the original DataFrame and concatenate with the new DataFrame\n",
    "    df_expanded = pd.concat([\n",
    "        df.iloc[:start_index+1],\n",
    "        df_to_append,\n",
    "        df.iloc[start_index+1:]\n",
    "    ]).reset_index(drop=True)\n",
    "\n",
    "    # Drop the merge indicator column\n",
    "    df_merged.drop(columns=['_merge'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24cb2a9-aa99-4985-8141-a26c5faee829",
   "metadata": {},
   "source": [
    "### Ploting \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2566f5d0-f8c5-4f68-83f7-7143bfb31be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_dataframe(*data_set): \n",
    "    data_set[0][1][0][\"Time\"] = data_set[0][1][0][\"Time\"].apply(lambda x: x.replace(year = 2000))\n",
    "    average_data = data_set[0][1][0].fillna(0).set_index('Time') # initiates first data\n",
    "    n = 1\n",
    "    for _,station in data_set[1:]: # skips first since already accounted for \n",
    "        for data in station:\n",
    "            data[\"Time\"] = data[\"Time\"].apply(lambda x: x.replace(year = 2000)) # to align all dataframes that spans over several years\n",
    "            average_data = average_data.add(data.set_index('Time').subtract(average_data, fill_value=0).div(n+1)) \n",
    "            n += 1\n",
    "        \n",
    "    average_data = average_data.reset_index()\n",
    "    return average_data\n",
    "\n",
    "def differense(*data_set): \n",
    "    avg_data = avg_dataframe(*data_set)\n",
    "    for _,station in data_set:\n",
    "        for data in station:\n",
    "            data[\"Time\"] = data[\"Time\"].apply(lambda x: x.replace(year = 2000))\n",
    "    new_data_set = [(key,[data.set_index(\"Time\").subtract(avg_data.set_index(\"Time\")).reset_index() for data in station]) for key,station in data_set]\n",
    "    return new_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c364627-f1b7-4a73-b438-3020b8a5fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_plot(diff_data,image_path,title = \"heatmap of data\",clear_plot = False):\n",
    "    \"\"\"\n",
    "        Makes a heatmap of diff_data -> list[pd.DataFrame]\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    cax = ax.matshow(\n",
    "        arr := np.array([data.TJM20.to_numpy() for data in diff_data.values()]).transpose(),\n",
    "        aspect='auto', cmap = \"seismic\"\n",
    "    )\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(arr.shape[1]),\n",
    "                  labels=list(diff_data.keys())\n",
    "    )\n",
    "\n",
    "    time_indexes = np.round(np.linspace(0, len((data_list := list(diff_data.values()))[0].Time.to_numpy()) - 1, 10)).astype(int)\n",
    "    ax.set_yticks(\n",
    "        time_indexes,\n",
    "        labels=np.array([d.strftime(\"%d-%m\") for d in data_list[0].Time.to_numpy()])[time_indexes]\n",
    "    )\n",
    "\n",
    "    C = len(diff_data.keys())/(2* len(nibio_id.keys()))\n",
    "    range_keys = np.linspace(0,len(diff_data.keys()),num = len(nibio_id.keys())+1)\n",
    "    # label the classes:\n",
    "    sec = ax.secondary_xaxis(location=0)\n",
    "    sec.set_xticks(range_keys[:-1] + C - 0.5, labels=['{}'.format(region) for region in nibio_id.keys()])\n",
    "    sec.tick_params('x', length=0)\n",
    "\n",
    "    # lines between the classes:\n",
    "    sec2 = ax.secondary_xaxis(location=0)\n",
    "    sec2.set_xticks(range_keys - 0.5, labels=[])\n",
    "    sec2.tick_params('x', length=10, width=1.5)\n",
    "    sec3 = ax.secondary_xaxis(location='top')\n",
    "    sec3.set_xticks(range_keys - 0.5, labels=[])\n",
    "    sec3.tick_params('x', length=10, width=1.5)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    #plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "    #         rotation_mode=\"anchor\")\n",
    "\n",
    "    cbar = fig.colorbar(cax, label='Gradient')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"\\nStations\")\n",
    "    ax.set_ylabel(\"Date\")\n",
    "    #ax.set_aspect('equal', adjustable='datalim')\n",
    "    plt.savefig(image_path)\n",
    "    if clear_plot:\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e6a7811-2e5f-4c57-861b-7766ecad45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polar_plot(diff_data,image_path,title = \"polar plot of data\",clear_plot = False): \n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "    list_diff_data = list(diff_data.items())\n",
    "    matrix = np.array([data.TJM20.to_numpy() for _,data in list_diff_data]).transpose()\n",
    "    ax.set_prop_cycle('color',[plt.cm.Blues(i) for i in np.linspace(0, 1, len(list_diff_data))])\n",
    "    for col_ind in range(len(list_diff_data)):\n",
    "        # Get the column vector\n",
    "        vector = matrix[:, col_ind]\n",
    "    \n",
    "        # Compute the angle for each element in the vector\n",
    "        angles = np.linspace(0, 2 * np.pi, len(vector), endpoint=False)\n",
    "    \n",
    "        # Plot the line connecting the origin to each element in the vector\n",
    "        ax.plot(angles, vector, label=f\"station {list_diff_data[col_ind]}\")\n",
    "\n",
    "    ax.grid(True)\n",
    "\n",
    "    #range_keys = np.linspace(0,len(diff_data.keys()),num = len(nibio_id.keys())+1)\n",
    "    time_indexes = np.linspace(0, 2*np.pi, 12, endpoint=False)\n",
    "    ax.set_xticks(\n",
    "        time_indexes,\n",
    "        labels=np.array([d.strftime(\"%d-%m\") for d in list(diff_data.values())[0].Time.to_numpy()])[(time_indexes* (list_diff_data[0][1]).shape[0]/(12)).astype(int)]\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, va='bottom')\n",
    "    #ax.legend()\n",
    "    plt.savefig(image_path)\n",
    "    if clear_plot:\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0774beef-4473-4626-95cd-fd62ccbba909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_plot(diff_data,image_path,title = \"naive plot of data\",clear_plot = False):\n",
    "    fig, ax = plt.subplots()\n",
    "    list_diff_data = list(diff_data.items())\n",
    "    matrix = np.array([data.TJM20.to_numpy() for _,data in list_diff_data]).transpose()\n",
    "    ax.set_prop_cycle('color',[plt.cm.Blues(i) for i in np.linspace(0, 1, len(list_diff_data))])\n",
    "    for col_ind in range(len(list_diff_data)):\n",
    "        # Get the column vector\n",
    "        vector = matrix[:, col_ind]\n",
    "    \n",
    "        # Compute the angle for each element in the vector\n",
    "        angles = range(len(vector))\n",
    "    \n",
    "        # Plot the line connecting the origin to each element in the vector\n",
    "        ax.plot(angles, vector, label=f\"station {list_diff_data[col_ind][0]}\")\n",
    "\n",
    "    ax.grid(True)\n",
    "\n",
    "    #range_keys = np.linspace(0,len(diff_data.keys()),num = len(nibio_id.keys())+1)\n",
    "    time_indexes = np.round(np.linspace(0, len(list_diff_data[0][1])-1, 12)).astype(int)\n",
    "    ax.set_xticks(\n",
    "        time_indexes,\n",
    "        labels=np.array([d.strftime(\"%d-%m\") for d in list(diff_data.values())[0].Time.to_numpy()])[time_indexes]\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, va='bottom')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    fig.set_size_inches(11.69, 8.27)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(image_path)\n",
    "    if clear_plot:\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8ac79-b239-4740-b623-638ab7478311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8115c9af-b0af-4f54-8489-bc7ff824ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "#def outlier_check_single_jump_center(x):\n",
    "#    real = x[1] # get target\n",
    "#    test = (x[-1] + x[0])/2 # linear interpolation of point\n",
    "#    test_diff = np.abs(x[-1] - x[0])/2\n",
    "#    diff = np.abs(real - test)\n",
    "#    if diff > 1.5*test_diff:\n",
    "#        return True\n",
    "#    return False\n",
    "\n",
    "#def outlier_check_RR(x):\n",
    "#    roll_data_right_2w = x.rolling(2*7*24,center = True)#\n",
    "#\n",
    "#    no_rain_2w = roll_data_right_2w.sum().to_numpy() < 1#mm\n",
    "#    \n",
    "#    greater_than_20 = x.to_numpy() > 20\n",
    "#    less_than_0 = x.to_numpy() < 0#\n",
    "#\n",
    "#    return ((no_rain_2w + greater_than_20 + less_than_0) > 0).fillna(value = False)\n",
    "\n",
    "def outlier_check(data,threshold,feature): #! må skrives om\n",
    "    detrended_data = data - data.rolling(24,center = True).mean()\n",
    "    big_jump_check = np.abs(zscore(detrended_data.to_numpy(), nan_policy='omit')) > threshold # simple outlier detection\n",
    "\n",
    "#    single_jump = data.rolling(3,center = True).apply(outlier_check_single_jump_center,raw=True)\n",
    "\n",
    "    change_wind = detrended_data.rolling(12).var() # looking at 12 hour intervall\n",
    "    snitt_var = np.mean(change_wind)\n",
    "    centered_changes = (change_wind - snitt_var).rolling(24).mean() # removes noise and centers\n",
    "    epsilon = [0.001,50,-50]\n",
    "    zero_change_check = change_wind.abs() < epsilon[0] # check for noe change\n",
    "    growth_change_check = change_wind.diff() > epsilon[1] # check for increasing changes\n",
    "    shrink_change_check = change_wind.diff() < epsilon[2] # check for decreasing changes\n",
    "\n",
    "    #match feature:\n",
    "    #    case \"RR\":\n",
    "    #        feature_check = outlier_check_RR(data)\n",
    "    #    case _:\n",
    "    #        feature_check = np.repeat(False,data.shape[0])\n",
    "\n",
    "    outlier_index = (zero_change_check \n",
    "                     + growth_change_check \n",
    "                     + shrink_change_check \n",
    "                     + big_jump_check \n",
    "                     #+ feature_check \n",
    "                     #+ single_jump\n",
    "                    ) > 0 # combines and scales to 0 or 1\n",
    "    return outlier_index.fillna(value = False)\n",
    "\n",
    "def naive_plot_na_values(diff_data,image_path: str,title: str = \"naive plot of data\",clear_plot: bool = False, feature: str= \"TJM20\", _recurs = False,threshold = 4, indicator = None):\n",
    "    \"\"\"\n",
    "        Data is:\n",
    "            - {\"station\": [y0,y1,y2,...]}\n",
    "        years in the same plot, station in different.\n",
    "\n",
    "        consider:\n",
    "            - Recursion if {\"S1\":[...],\"S2\":[...],...}\n",
    "    \"\"\"\n",
    "    head, _,tail = image_path.rpartition(\".\")\n",
    "    if len(diff_data)>1:\n",
    "        for key in diff_data:\n",
    "            new_title = \"{}_k{}_f{}\".format(title,key,feature)\n",
    "            naive_plot_na_values({key:diff_data[key]},image_path=image_path,title = new_title,clear_plot = clear_plot,feature = feature, _recurs = True)\n",
    "        return\n",
    "\n",
    "    # after this point {\"Station\":[y0,y1,y2,...]}\n",
    "    list_data = list(diff_data.items()) # [(station,[...])]\n",
    "    years = list_data[0][1]\n",
    "    n_years = len(years)\n",
    "    sta_id = list_data[0][0]\n",
    "        \n",
    "    fig, ax = plt.subplots(nrows=n_years+1) # ax = [...]\n",
    "\n",
    "    #matrix = np.array([data.loc[:,feature].to_numpy() for _,data in list_data[1]]).transpose() #? nyttig?\n",
    "\n",
    "    first_year = years[0].Time[pd.Interval(0,5880).mid].year # pick an element that is garanteed to be in the year\n",
    "    #print(sta_id)\n",
    "    for i,(ax_i, colour_id) in enumerate(zip(ax[:-1],np.linspace(0, 1, n_years))):\n",
    "        #print(first_year + i)\n",
    "        # Get the column vector\n",
    "        vec_data = years[i].loc[:, feature]\n",
    "        vector = vec_data.to_numpy().ravel() # convert to vector\n",
    "\n",
    "        na_pos = vec_data.isna().to_numpy()\n",
    "\n",
    "        vector_inter = vec_data.bfill()\n",
    "\n",
    "        outlier_index = outlier_check(vector_inter, threshold,feature)\n",
    "\n",
    "        #na_indexes = years[i].loc[:, feature].isna().to_numpy()\n",
    "    \n",
    "        # Compute the angle for each element in the vector\n",
    "        #angles = range(len(vector))\n",
    "    \n",
    "        # Plot the line connecting the origin to each element in the vector\n",
    "\n",
    "\n",
    "        #limits = ax_i.get_ylim() # (bot,top)\n",
    "        bottom_array = np.repeat(np.min(vector_inter),repeats=5880)\n",
    "        top_array = np.repeat(np.max(vector_inter),repeats=5880)\n",
    "        \n",
    "        #vector_inter = vec_data.bfill()\n",
    "        #vector_inter[np.logical_not(outlier_index)] = np.nan\n",
    "        b_array = bottom_array.copy()\n",
    "        t_array = top_array.copy()\n",
    "        b_array[np.logical_not(outlier_index)] = np.nan\n",
    "        t_array[np.logical_not(outlier_index)] = np.nan\n",
    "        ax_i.fill_between(range(0,5880),t_array,b_array,color=\"yellow\",linewidth=0)#,s = 0.07,marker = \".\",linewidth=0,color = \"yellow\")\n",
    "\n",
    "        ax_i.set_ylabel(\"y{}\".format(first_year + i))\n",
    "        ax_i.set_xticks([i for i in range(0,5880,500)],[i for i in range(0,5880,500)])\n",
    "        ax_i.scatter(range(0,5880),vector,s = 0.07,marker = \"*\",linewidth=0)#,color = plt.cm.Blues(colour_id))\n",
    "        ax[-1].scatter(range(0,5880),vector,s = 0.07,marker = \"*\",linewidth=0,color = plt.cm.plasma(colour_id),label = first_year + i)\n",
    "        #vector_inter = vec_data.bfill()\n",
    "        #vector_inter[np.logical_not(na_pos)] = np.nan\n",
    "        b_array = bottom_array.copy()\n",
    "        t_array = top_array.copy()\n",
    "        b_array[np.logical_not(na_pos)] = np.nan\n",
    "        t_array[np.logical_not(na_pos)] = np.nan\n",
    "        ax_i.fill_between(range(0,5880),t_array,b_array,color = \"red\",linewidth=0)#,s = 0.07,marker = \".\",linewidth=0,color = \"red\")\n",
    "\n",
    "        # singular Nan-s\n",
    "        sing_nan_index = np.where((na_pos[:-2] == False) & (na_pos[1:-1] == True) & (na_pos[2:] == False))[0] + 1\n",
    "        x_coord = []\n",
    "        y_min_coord = []\n",
    "        y_max_coord = []\n",
    "        for x in sing_nan_index:\n",
    "            x_coord.extend([x-0.5,x+0.5,np.nan])\n",
    "            y_min_coord.extend([bottom_array[0],bottom_array[0],np.nan])\n",
    "            y_max_coord.extend([top_array[0],top_array[0],np.nan])\n",
    "        ax_i.fill_between(x_coord,y_min_coord,y_max_coord,color = \"red\",linewidth=0)\n",
    "\n",
    "        feature_na_data = terskel_data.loc[(terskel_data[\"station\"] == int(sta_id)) & (terskel_data[\"year\"] == (first_year + i)),[feature,small_f := \"|{}|≤{}\".format(feature,terskel),big_f := \"|{}|>{}\".format(feature,terskel)]].to_numpy().ravel() # pandas tabel\n",
    "\n",
    "        y_min,y_max = ax_i.get_ylim()\n",
    "        sec = ax_i.secondary_yaxis(location=\"right\")\n",
    "        sec.set_yticks(np.linspace(y_min,y_max,num=5)[1:-1],labels=[\"tot:{}h\".format(feature_na_data[0]),\"{}≥t:{}\".format(terskel,feature_na_data[1]),\"{}<t:{}\".format(terskel,feature_na_data[2])])\n",
    "\n",
    "        \n",
    "\n",
    "    #fig.grid(True)\n",
    "\n",
    "    #range_keys = np.linspace(0,len(diff_data.keys()),num = len(nibio_id.keys())+1)\n",
    "    time_indexes = np.round(np.linspace(0,5880,12, endpoint=False)).astype(int)\n",
    "    ax[-1].set_xticks(\n",
    "        time_indexes,\n",
    "        labels=np.array([d.strftime(\"%d-%m\") for d in years[0].Time.to_numpy()])[time_indexes]\n",
    "    )\n",
    "    ax[-1].set_ylabel(\"overlap\")\n",
    "    #ax[-1].legend(loc = (1.3,1))\n",
    "\n",
    "    ax[0].set_title(title, va='bottom')\n",
    "    plt.setp(ax[-1].get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    fig.set_size_inches(11.69, 8.27) # \n",
    "    plt.tight_layout()\n",
    "    if _recurs:\n",
    "        plt.savefig(\"{}_k{}_f{}.{}\".format(head,list(diff_data.keys())[0],feature,tail))\n",
    "    else:\n",
    "        plt.savefig(image_path)\n",
    "    if clear_plot:\n",
    "        plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1baebb62-61d4-4211-8244-c0bc6b8d5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_plot_cumsum_na_values(diff_data,image_path: str,title: str = \"naive plot of data\",clear_plot: bool = False, feature: str= \"RR\", _recurs = False,threshold = 4):\n",
    "    \"\"\"\n",
    "        Data is:\n",
    "            - {\"station\": [y0,y1,y2,...]}\n",
    "        years in the same plot, station in different.\n",
    "\n",
    "        consider:\n",
    "            - Recursion if {\"S1\":[...],\"S2\":[...],...}\n",
    "    \"\"\"\n",
    "    head, _,tail = image_path.rpartition(\".\")\n",
    "    if len(diff_data)>1:\n",
    "        for key in diff_data:\n",
    "            new_title = \"{}_k{}_f{}\".format(title,key,feature)\n",
    "            naive_plot_cumsum_na_values({key:diff_data[key]},image_path=image_path,title = new_title,clear_plot = clear_plot,feature = feature, _recurs = True)\n",
    "        return\n",
    "\n",
    "    # after this point {\"Station\":[y0,y1,y2,...]}\n",
    "    list_data = list(diff_data.items()) # [(station,[...])]\n",
    "    years = list_data[0][1]\n",
    "    n_years = len(years)\n",
    "    sta_id = list_data[0][0]\n",
    "        \n",
    "    fig, ax = plt.subplots(nrows=n_years+1) # ax = [...]\n",
    "\n",
    "    #matrix = np.array([data.loc[:,feature].to_numpy() for _,data in list_data[1]]).transpose() #? nyttig?\n",
    "\n",
    "    first_year = years[0].Time[pd.Interval(0,5880).mid].year # pick an element that is garanteed to be in the year\n",
    "    \n",
    "    for i,(ax_i, colour_id) in enumerate(zip(ax[:-1],np.linspace(0, 1, n_years))):\n",
    "        # Get the column vector\n",
    "        vec_data = years[i].loc[:, feature].cumsum()\n",
    "        vector = vec_data.to_numpy().ravel() # convert to vector\n",
    "\n",
    "        na_pos = vec_data.isna().to_numpy()\n",
    "\n",
    "        vector_inter = vec_data.bfill()\n",
    "\n",
    "        #na_indexes = years[i].loc[:, feature].isna().to_numpy()\n",
    "    \n",
    "        # Compute the angle for each element in the vector\n",
    "        #angles = range(len(vector))\n",
    "        bottom_array = np.repeat(np.min(vector_inter),repeats=5880)\n",
    "        top_array = np.repeat(np.max(vector_inter),repeats=5880)\n",
    "    \n",
    "        # Plot the line connecting the origin to each element in the vector\n",
    "        ax_i.set_ylabel(\"y{}\".format(first_year + i))\n",
    "        #ax_i.set_xticklabels([])\n",
    "        ax_i.fill_between(range(0,5880),vector)#,s = 0.07,)#,color = plt.cm.Blues(colour_id))\n",
    "        ax[-1].scatter(range(0,5880),vector,s = 0.07,marker = \"*\",linewidth=0,color = plt.cm.plasma(colour_id),label = first_year + i)\n",
    "        bottom_array[np.logical_not(na_pos)] = np.nan\n",
    "        top_array[np.logical_not(na_pos)] = np.nan\n",
    "        ax_i.fill_between(range(0,5880),top_array,bottom_array,linewidth=0,color = \"red\")#(range(0,5880),vector_inter,s = 0.07,marker = \".\",linewidth=0,color = \"red\")\n",
    "        \n",
    "        #vector_inter = vec_data.ffill()\n",
    "        #outlier_index = outlier_check(vector_inter,threshold,feature)\n",
    "        #vector_inter[np.logical_not(outlier_index)] = np.nan\n",
    "        #ax_i.scatter(range(0,5880),vector_inter,s = 0.07,marker = \".\",linewidth=0,color = \"yellow\")\n",
    "        #ax_i.plot(np.arange(len(vector)), na_indexes,\"r.\")\n",
    "        \n",
    "        sing_nan_index = np.where((na_pos[:-2] == False) & (na_pos[1:-1] == True) & (na_pos[2:] == False))[0] + 1\n",
    "        x_coord = []\n",
    "        y_min_coord = []\n",
    "        y_max_coord = []\n",
    "        for x in sing_nan_index:\n",
    "            x_coord.extend([x-0.5,x+0.5,np.nan])\n",
    "            y_min_coord.extend([bottom_array[0],bottom_array[0],np.nan])\n",
    "            y_max_coord.extend([top_array[0],top_array[0],np.nan])\n",
    "        ax_i.fill_between(x_coord,y_min_coord,y_max_coord,color = \"red\",linewidth=0)\n",
    "        feature_na_data = terskel_data.loc[(terskel_data[\"station\"] == int(sta_id)) & (terskel_data[\"year\"] == (first_year + i)),[feature,small_f := \"|{}|≤{}\".format(feature,terskel),big_f := \"|{}|>{}\".format(feature,terskel)]].to_numpy().ravel() # pandas tabel\n",
    "\n",
    "        y_min,y_max = ax_i.get_ylim()\n",
    "        sec = ax_i.secondary_yaxis(location=\"right\")\n",
    "        sec.set_yticks(np.linspace(y_min,y_max,num=5)[1:-1],labels=[\"tot:{}h\".format(feature_na_data[0]),\"{}≥t:{}\".format(terskel,feature_na_data[1]),\"{}<t:{}\".format(terskel,feature_na_data[2])])\n",
    "\n",
    "    #fig.grid(True)\n",
    "\n",
    "    #range_keys = np.linspace(0,len(diff_data.keys()),num = len(nibio_id.keys())+1)\n",
    "    time_indexes = np.round(np.linspace(0,5880,12, endpoint=False)).astype(int)\n",
    "    ax[-1].set_xticks(\n",
    "        time_indexes,\n",
    "        labels=np.array([d.strftime(\"%d-%m\") for d in years[0].Time.to_numpy()])[time_indexes]\n",
    "    )\n",
    "    ax[-1].set_ylabel(\"overlap\")\n",
    "\n",
    "    ax[0].set_title(title, va='bottom')\n",
    "    plt.setp(ax[-1].get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    fig.set_size_inches(11.69, 8.27) # \n",
    "    plt.tight_layout()\n",
    "    if _recurs:\n",
    "        plt.savefig(\"{}_k{}_f{}.{}\".format(head,list(diff_data.keys())[0],feature,tail))\n",
    "    else:\n",
    "        plt.savefig(image_path)\n",
    "    if clear_plot:\n",
    "        plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68cccb02-911e-4f9d-8721-84cce9fa8213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = imputed_nibio_data.shave_top_layer().flatten(return_key = True) # [(key, value)]\n",
    "#print(all_data)\n",
    "#diff_data = differense(*all_data)\n",
    "#diff_data = dict(diff_data)\n",
    "#diff_data = imputed_nibio_data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560cc744-b7bc-412a-968c-8c57632e4bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [\"TJM20\",\"TJM10\",\"TM\"]:\n",
    "    naive_plot_na_values(dict(all_data),PLOT_PATH + \"Plot_test_naive_nan.pdf\",feature = f,title = \"Untreated data\")\n",
    "naive_plot_cumsum_na_values(dict(all_data),PLOT_PATH + \"Plot_test_naive_nan.pdf\",title = \"Untreated data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8547a5c-b7b4-4a78-b82a-bc1f94c11173",
   "metadata": {},
   "source": [
    "## Imputing test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f9ace75-a82f-455d-adf9-57fceb0c4204",
   "metadata": {},
   "source": [
    "\n",
    "class STL_imputation:\n",
    "    \"\"\"\n",
    "        Data to be imputed by decomposing it first then imuting missing values.\n",
    "        Kwarg gets passed into MSTL.\n",
    "\n",
    "        Strategy: Since MSTL cant handle Nan I will have to decompose the part I can and use imputation to connect the pices, then return the computed series.\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator = None, kwarg = {}):\n",
    "        \"\"\"\n",
    "            estiamtor needs the 'fit_transform' and if estimator is a list it\n",
    "            needs to be less or equal to the number of components (Trend, [Sesonal components], Residual),\n",
    "            kwarg gets passed to estimator\n",
    "        \"\"\"\n",
    "        assert estimator is not None, ValueError(\"Needs to spesify estimator\")\n",
    "        self.estimator = estimator\n",
    "        self.params = kwarg\n",
    "\n",
    "    def fit_transform(self,data,target = None):\n",
    "        decompose = MSTL(data,**self.params) # does not work with nan\n",
    "\n",
    "class Pandas_imputation:\n",
    "    def __init__(self, method = \"fillna\",kwargs = {}):\n",
    "        self.params = kwargs\n",
    "        self.method = method\n",
    "\n",
    "    def fit_transform(self, data,target = None):\n",
    "        match self.method:\n",
    "            case \"fillna\":\n",
    "                return data.fillna(**self.params)\n",
    "            case \"interpolate\":\n",
    "                return data.interpolate(**self.params)\n",
    "            case \"ffill\":\n",
    "                return data.ffill(**self.params)\n",
    "            case \"bfill\":\n",
    "                return data.bfill(**self.params)\n",
    "            case \"mean\":\n",
    "                return data.fillna(data.mean(),**self.params)\n",
    "            case _:\n",
    "                raise ValueError(\"Did not reqonse '{}'\".format(self.method))\n",
    "\n",
    "def find_non_nan_ranges(df):\n",
    "    \"\"\"\n",
    "    Finds the ranges of indexes where rows do not contain NaNs in the DataFrame.\n",
    "    Assumes there is a 'Time' column with timestamps.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with NaNs.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: List of (start, end) index ranges where rows do not contain NaNs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    non_nan_ranges = []\n",
    "    start_idx = None\n",
    "\n",
    "    # Iterate over rows\n",
    "    for idx, row in df.iterrows():\n",
    "        if not row.isna().any():\n",
    "            # If the row does not contain NaNs\n",
    "            if start_idx is None:\n",
    "                # If this is the start of a new range\n",
    "                start_idx = idx\n",
    "        else:\n",
    "            # If the row contains NaNs\n",
    "            if start_idx is not None:\n",
    "                # If this is the end of a range\n",
    "                non_nan_ranges.append((start_idx, idx - 1))\n",
    "                start_idx = None\n",
    "\n",
    "    # Check if the last range is still open\n",
    "    if start_idx is not None:\n",
    "        non_nan_ranges.append((start_idx, df.index[-1]))\n",
    "\n",
    "    return non_nan_ranges\n",
    "\n",
    "def create_endpoints(start, end, length = None,size = None): \n",
    "    \"\"\"\n",
    "        Creates a tuple with start and end points. Start points are chosen at random and endpoints is calculated b ased on length\n",
    "        output: [(s,e),...]\n",
    "    \"\"\"\n",
    "    if length is None:\n",
    "        length = end - start + 1\n",
    "    if size is None:\n",
    "        size = end - start +1\n",
    "    batch = [ (i+start, i +start+ length) for i in  np.random.choice(end-start+1,replace=False,size = min(size,end-start+1)) ]\n",
    "    return batch\n",
    "\n",
    "def score(y_pred, y_true): \n",
    "    return root_mean_squared_error(y_pred,y_true)\n",
    "\n",
    "all_data = [df.loc[:,[\"TM\",\"TJM10\",\"TJM20\",\"RR\"]] for df in imputed_nibio_data.flatten()] # [DataFrame]\n",
    "\n",
    "imputation_method = [\n",
    "    (Pandas_imputation,{\"method\":\"fillna\",\"kwargs\":{\"value\":0}}),\n",
    "    (Pandas_imputation,{\"method\":\"interpolate\",\"kwargs\":{\"method\":\"linear\"}}),\n",
    "    (Pandas_imputation,{\"method\":\"ffill\",\"kwargs\":{}}),\n",
    "    (Pandas_imputation,{\"method\":\"bfill\",\"kwargs\":{}}),\n",
    "    (Pandas_imputation,{\"method\":\"mean\",\"kwargs\":{}})\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d0231ca-ea82-49ba-ad7d-9c130844a2c4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import MSTL\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "alpha = all_data[0].TJM10.to_numpy().ravel().copy()\n",
    "alpha[2174] = 13.8\n",
    "\n",
    "print(ARIMA(alpha,order=(0,1,0)).fit().summary())\n",
    "\n",
    "MSTL(alpha, periods=[24]).fit().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0bb4526a-d25d-4452-807c-dc48b1511ff1",
   "metadata": {},
   "source": [
    "imput_res = [\n",
    "    {\"name\":method.__name__,\"model\":method,\"Score\":np.inf,\"config\":{},\"length\":0} for method,_ in imputation_method\n",
    "]\n",
    "\n",
    "size_of_data = 0.3# 0.4\n",
    "number_of_allowed_fails = 15\n",
    "fail_threshold = 3\n",
    "n_tests_per_length_max = 10\n",
    "\n",
    "print(\"Beginning imputation test\")\n",
    "for t, (method,method_arguments) in enumerate(imputation_method): \n",
    "    avg_error = 0\n",
    "    total_length = 0\n",
    "    length_metric = {}\n",
    "    for station_index, station in enumerate(np.random.choice(range(len(all_data)),size=len(all_data),replace = False)): # we randomly select a dataset to check\n",
    "        current_data = all_data[station]\n",
    "        max_range_1_unit = 0\n",
    "        print(\"Progress\",(t*len(imputation_method)+station_index) / (int(size_of_data*len(all_data))*len(imputation_method)),end=\"\\r\")\n",
    "        if station_index >= int(size_of_data*len(all_data)): # checks if I have gone through a portion of the data\n",
    "            break\n",
    "        for span_start, span_end in find_non_nan_ranges(current_data): \n",
    "            for i in range(span_start+1,span_end-1): \n",
    "                length_score = 0\n",
    "                length_n = 0\n",
    "                for j,k in create_endpoints(span_start+1,span_end-2,length = span_end - i, size = n_tests_per_length_max):\n",
    "                    test_data = current_data.copy()\n",
    "                    for feature in test_data.columns:\n",
    "                        if not(any(test_data.loc[j:k,feature].isna().to_numpy().ravel())) and not(any(current_data.loc[j:k,feature].isna().to_numpy().ravel())):\n",
    "                            test_data.loc[j:k,feature] = np.nan\n",
    "                            imputet = method(**method_arguments).fit_transform(test_data.loc[:,feature])\n",
    "                            if any(imputet.isna().to_numpy().ravel()): \n",
    "                                continue\n",
    "                            #total_scores.append(calc_score := score(imputet[j:k+1],current_data.loc[j:k,feature])*(k-j+1)) # calculates a score\n",
    "                            calc_score = score(imputet[j:k+1],current_data.loc[j:k,feature])\n",
    "                            avg_error += calc_score*(span_end - i)\n",
    "                            length_score += calc_score*(span_end - i)\n",
    "                            total_length += span_end - i\n",
    "                            length_n += span_end - i\n",
    "                if i in length_metric:\n",
    "                    length_metric[i].update({\n",
    "                        \"score\":length_metric[i][\"score\"] + length_score,\n",
    "                        \"n\":length_metric[i][\"n\"] + length_n\n",
    "                    })\n",
    "                else:\n",
    "                    length_metric[i] = {\n",
    "                        \"score\":length_score,\n",
    "                        \"n\":length_n\n",
    "                    }\n",
    "    if (s := avg_error/total_length) <= imput_res[t][\"Score\"]: \n",
    "        imput_res[t].update({\n",
    "            \"Score\":s,\n",
    "            \"length_metric\":length_metric\n",
    "            #\"detail\":{\"mse\":total_scores,\"i\":range(1,span_end-1-span_start)}\n",
    "        })\n",
    "                \n",
    "            \n",
    "print()\n",
    "print(\"Ended imputation test\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf4555ff-90ce-48d9-ba24-7698c7fe518d",
   "metadata": {},
   "source": [
    "imput_res, min(imput_res,key=lambda x: x[\"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2958df5-d9eb-4d65-8c96-a842320b3a79",
   "metadata": {},
   "source": [
    "## Choose best method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7ed005b-eceb-43c9-a236-ab5704ae1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nibio_data_ungroup = DFL.DataFileLoader(RESULT_PATH,r\"weather_data_hour_stID(\\d{1,3})_y(\\d{4}).csv\",_iter_key = True)\n",
    "nibio_data_ungroup.load_data(names = [\"Time\",\"TM\",\"RR\",\"TJM10\",\"TJM20\"])\n",
    "nibio_data = nibio_data_ungroup.group_layer(nibio_id)\n",
    "\n",
    "nibio_data_raw_ungroup = DFL.DataFileLoader(RESULT_PATH,r\"weather_data_raw_hour_stID(\\d{1,3})_y(\\d{4}).csv\",_iter_key = True)\n",
    "nibio_data_raw_ungroup.load_data(names = [\"Time\",\"TM\",\"RR\",\"TJM10\",\"TJM20\"])\n",
    "nibio_data_raw = nibio_data_raw_ungroup.group_layer(nibio_id)\n",
    "\n",
    "def dataframe_merge_func(x,y):\n",
    "        y.iloc[y.iloc[:,1].notna() & (y.iloc[:,1] <= 0),2] = pd.NA\n",
    "        x.iloc[0:y.shape[0],2] = y.iloc[0:y.shape[0],2]\n",
    "        return x\n",
    "\n",
    "imputed_nibio_data_cleaned = nibio_data.combine(nibio_data_raw,merge_func = dataframe_merge_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfc48c-a5dc-46e2-b77c-23c7ef47e21c",
   "metadata": {},
   "source": [
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17396e0-25c5-45a1-b11c-ac82f23aa805",
   "metadata": {},
   "source": [
    "## Removes outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f075c3a-00ab-42b7-9100-eb04744b9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = imputed_nibio_data_cleaned.shave_top_layer().flatten(return_key = True) # [(key, value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4279ba4-6745-4b78-86d9-4fdb37cc65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_pronto = {\n",
    "    \"15\": {\n",
    "        \"TM\": {\n",
    "            2015: [[2293,2301]],\n",
    "            2021: [[1230,1232]]\n",
    "        },\n",
    "        \"TJM10\": {\n",
    "            2015: [[2293,2301]],\n",
    "            2021: [[1230,1232]]\n",
    "        },\n",
    "        \"TJM20\": {\n",
    "            2015: [[2293,2301]],\n",
    "            2021: [[1230,1232]]\n",
    "        }\n",
    "    },\n",
    "    \"17\": {\n",
    "        \"TJM10\": {\n",
    "            2014: [[0,5879]], # Fjerner hele året\n",
    "            2015: [[0,5879]], # Fjerner hele året\n",
    "            2016: [[0,2500]], # Fjerner halve året\n",
    "            2020: [[5201,5879]],\n",
    "            2021: [[0,5879]]\n",
    "        }, \n",
    "        \"TJM20\": {\n",
    "            2014: [[0,5879]], # Fjerner hele året\n",
    "            2015: [[0,5879]], # Fjerner hele året\n",
    "            2016: [[0,2500]], # Fjerner halve året\n",
    "            2020: [[5201,5879]],\n",
    "            2021: [[0,5879]]\n",
    "        },\n",
    "        \"TM\": {\n",
    "            2014: [[0,5879]], # Fjerner hele året\n",
    "            2015: [[0,5879]], # Fjerner hele året\n",
    "            2016: [[0,2500]], # Fjerner halve året\n",
    "            2020: [[5201,5879]],\n",
    "            2021: [[0,5879]] # fjerner hele året\n",
    "        }\n",
    "    },\n",
    "    \"26\": {\n",
    "        \"TJM10\": {\n",
    "            2017: [[5503,5823],[4622,4692]]\n",
    "        }\n",
    "    },\n",
    "    \"30\": {\n",
    "        \"TJM10\": {\n",
    "            2016: [[200,1187]]\n",
    "        },\n",
    "        \"TJM20\": {\n",
    "            2016: [[200,1187]]\n",
    "        },\n",
    "        \"TM\": {\n",
    "            2016: [[200,1187]]\n",
    "        }\n",
    "    },\n",
    "    \"34\": {\n",
    "        \"TM\": {\n",
    "            2021: [[2635,2649]],\n",
    "            2022: [[2410,2433],[2768,2786]]\n",
    "        },\n",
    "        \"TJM10\": {\n",
    "            2021: [[2635,2649]],\n",
    "            2022: [[2410,2433],[2768,2786]]\n",
    "        },\n",
    "        \"TJM20\": {\n",
    "            2021: [[2635,2649]],\n",
    "            2022: [[2410,2433],[2768,2786]]\n",
    "        }\n",
    "    },\n",
    "    \"41\": {\n",
    "        \"TJM20\":{\n",
    "            2014: [[1008,1453]],\n",
    "            2016: [[3084,3228]],\n",
    "            2019: [[3230,4018]],\n",
    "            2020: [[5342,5350],[5646,5798]]\n",
    "        },\n",
    "        \"TJM10\":{\n",
    "            2014: [[962,1453]],\n",
    "            2016: [[3084,3228]],\n",
    "            2019: [[3230,4018]],\n",
    "            2020: [[5342,5350],[5646,5798]]}\n",
    "    }, \n",
    "    \"42\": {\n",
    "        \"TM\":{\n",
    "            2019:[[2989,3138]]\n",
    "        },\n",
    "        \"TJM20\":{\n",
    "            2019:[[2989,3138]]\n",
    "        },    \n",
    "        \"TJM10\":{\n",
    "            2019:[[2989,3138]]\n",
    "        }\n",
    "    },\n",
    "    \"50\": {\n",
    "        \"TJM20\":{\n",
    "            2014:[[2085,2110],[3939,3965],[5785,5879]],\n",
    "            2017:[[3699,4334]]\n",
    "        },\n",
    "        \"TJM10\":{\n",
    "            2014:[[2085,2110],[3939,3965],[5785,5879]]\n",
    "        },\n",
    "        \"TM\":{\n",
    "            2014:[[2085,2110],[3939,3965],[5785,5879]]\n",
    "        },\n",
    "    },\n",
    "    \"52\": {\n",
    "        \"TJM20\":{\n",
    "            2017:[[3786,3856]]\n",
    "        },    \n",
    "        \"TJM10\":{\n",
    "            2017:[[3786,3856]]\n",
    "        },    \n",
    "        \"TM\":{\n",
    "            2017:[[3786,3856]]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "all_data_dict = dict(all_data)\n",
    "clone_all_data_dict = all_data_dict.copy()\n",
    "\n",
    "for station in all_data_dict:\n",
    "    for ind,_ in enumerate(all_data_dict[station]):\n",
    "        for feat in all_data_dict[station][ind].columns:\n",
    "            all_data_dict[station][ind].loc[:,feat] = all_data_dict[station][ind].loc[:,feat].infer_objects(copy=False).interpolate(limit= 3 if feat == \"TM\" else 5,limit_direction=\"forward\") # bare første ende\n",
    "\n",
    "for station in remove_pronto:\n",
    "    for feat in remove_pronto[station]:\n",
    "        for yr in remove_pronto[station][feat]:\n",
    "            for inter in remove_pronto[station][feat][yr]:\n",
    "                all_data_dict[station][yr - 2014].loc[inter[0]:inter[1],feat] = np.nan\n",
    "\n",
    "for station in all_data_dict: \n",
    "    current_station = {}\n",
    "    for year,data in enumerate(all_data_dict[station]): \n",
    "        current_station[str(2014+year)] = data\n",
    "    clone_all_data_dict[station] = current_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a98b16b0-af4a-4b78-a978-ec453d589917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'naive_plot_na_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTJM20\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTJM10\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTM\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mnaive_plot_na_values\u001b[49m(all_data_dict,PLOT_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlot_test_naive_nan_treated.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m,feature \u001b[38;5;241m=\u001b[39m f,title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtreated data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m naive_plot_cumsum_na_values(all_data_dict,PLOT_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlot_test_naive_nan_treated.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m,title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtreated data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'naive_plot_na_values' is not defined"
     ]
    }
   ],
   "source": [
    "for f in [\"TJM20\",\"TJM10\",\"TM\"]:\n",
    "    naive_plot_na_values(all_data_dict,PLOT_PATH + \"Plot_test_naive_nan_treated.pdf\",feature = f,title = \"treated data\")\n",
    "naive_plot_cumsum_na_values(all_data_dict,PLOT_PATH + \"Plot_test_naive_nan_treated.pdf\",title = \"treated data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0d401-9259-44a3-bc7c-c5d6caa6892b",
   "metadata": {},
   "source": [
    "## Sending treated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa2a2249-ebd8-4d15-bf45-dd48b88f41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = DFL.DataFileLoader().load_dict(clone_all_data_dict).group_layer(nibio_id)\n",
    "\n",
    "imputed_data.dump(METADATA_PRELOAD_DATA_PATH + \"weatherdata_cleaned.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7006cf1a-9e95-4119-ab25-e29de34547ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in clone_all_data_dict.keys():\n",
    "    for yt,year in clone_all_data_dict[station].items():\n",
    "        year.to_csv(AUGME_PATH + \"weather_data_hour_stID{}_y{}.csv\".format(station,yt), sep=\",\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8247e1-06a4-4788-a96e-e286525238ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
