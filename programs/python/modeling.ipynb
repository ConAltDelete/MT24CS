{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002fa039-b352-4270-ad6e-b64938c6ad64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Data visualisation\n",
    "\n",
    "We start by importing the data and do analisys on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f2b316c-c368-4c90-8502-10f354d296f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels as sm\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation \n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout, Conv2DTranspose\n",
    "from tensorflow.keras.layers import concatenate, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "#sklearn → model trening\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics         import accuracy_score\n",
    "\n",
    "#sklearn → data treatment\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# path definitions\n",
    "\n",
    "ROOT = \"../../\"\n",
    "\n",
    "DATA_PATH = ROOT + \"data/\"\n",
    "\n",
    "DATA_INFO = DATA_PATH + \"info/\"\n",
    "DATA_INFO_NIBIO_FILE = DATA_INFO  + \"lmt.nibio.csv\"\n",
    "DATA_INFO_FROST_FILE = DATA_INFO + \"Frost_stations.csv\"\n",
    "DATA_FILE_SOIL_STATIONS = DATA_INFO + \"'Stasjonsliste jordtemperatur modellering.xlsx'\"\n",
    "\n",
    "DATA_COLLECTION = DATA_PATH + \"raw_data/\"\n",
    "DATA_COLLECTION_STAT = DATA_COLLECTION + \"Veret paa Aas 2013- 2017/\" # pattern -> 'Veret paa Aas 2013- 2017/Veret paa Aas {YYYY}.pdf'\n",
    "DATA_COLLECTION_TIME = DATA_COLLECTION + \"Time 2013- 2023/\" # pattern -> Time{YYYY}.xlsx\n",
    "DATA_COLLECTION_NIBIO = DATA_COLLECTION + \"nibio/\" # pattern -> weather_data_hour_stID{id}_y{year}.csv\n",
    "\n",
    "# ID definitions\n",
    "\n",
    "station_names = pd.read_csv(DATA_INFO_NIBIO_FILE,\n",
    "                          header=0,\n",
    "                          index_col = \"ID\")\n",
    "\n",
    "nibio_id = {\n",
    "    \"Innlandet\" : [11,18,26,27],\n",
    "    \"Trøndelag\" : [15,57,34,39],\n",
    "    \"Østfold\" : [37,41,52,118],\n",
    "    \"Vestfold\" : [30,38,42,50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616206e-9e44-4114-ab44-619b3954ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definitions\n",
    "\n",
    "import os.path\n",
    "\n",
    "def table2Latex(table, dir_path, file_name, header = \"\", append = False):\n",
    "    if os.path.isfile(dir_path + file_name + \".tex\") & append:\n",
    "        file = open(dir_path+filename+\".tex\",\"a\", encoding=\"utf-8\")\n",
    "    else:\n",
    "        file = open(dir_path+filename+\".tex\",\"w\", encoding=\"utf-8\")\n",
    "\n",
    "    file.write(r\"\\begin{tabular}{|l|\" + [\"c\" for _ in range(len(header))].join(\"|\") + \"|}\")\n",
    "    if header != \"\":\n",
    "        file.write(header.join(\"&\") + r\"\\\\\\hline\")\n",
    "    for row in table:\n",
    "        file.write(row.join(\"&\") + r\"\\\\\\hline\")\n",
    "    file.write(r\"\\end{tabular}\")\n",
    "\n",
    "\"\"\"\n",
    "file_name.nibio = function(station_id, year, path = NULL){\n",
    "    if(is.null(path)){\n",
    "        pattern = DATA_COLLECTION_NIBIO + \"weather_data_hour_stID\" + station_id + \"_y\" + year + \".csv\"\n",
    "    } else {\n",
    "        pattern = sprintf(path,station_id,year)\n",
    "    }\n",
    "    return(pattern)\n",
    "}\n",
    "\n",
    "data.nibio = function(station_id,year, path = NULL){\n",
    "    path = file_name.nibio(station_id,year, path = path)\n",
    "    data_nibio = read.csv(path,\n",
    "                       header=T, col.names = c(\"Time\",\"TM\",\"RR\",\"TJM10\",\"TJM20\"))\n",
    "    data_nibio = mutate(data_nibio,across(\n",
    "                                    \"Time\",\n",
    "                                  str2date))\n",
    "    data_nibio = column_to_rownames(data_nibio, var = \"Time\")\n",
    "    data_nibio = mutate_at(data_nibio,c(\"TM\",\"RR\",\"TJM10\",\"TJM20\"), as.numeric)\n",
    "    return(data_nibio)\n",
    "}\n",
    "na.interpol.cust = function(data, maxgap = Inf, n.p, \n",
    "                             s.window = 10, alg.option = \"linear\"){\n",
    "    data.decomp = stlplus::stlplus(data,n.p = n.p, s.window = s.window)\n",
    "    data.new = rep(0,length.out = length(data))\n",
    "    for(part in c(\"seasonal\", \"trend\", \"remainder\")){\n",
    "        data.new = data.new + na_interpolation(data.decomp$data[,part],\n",
    "                                                maxgap=maxgap,\n",
    "                                                option = alg.option)\n",
    "    }\n",
    "    return(data.new)\n",
    "}\n",
    "str2date = function(x) {\n",
    "    return(as.POSIXlt(x + \"00\",\n",
    "                      format = \"%Y-%m-%d %H:%M:%S%z\",\n",
    "                      tz=\"GMT\"))\n",
    "}\n",
    "\n",
    "na.interplol.kal =function(data, maxgap = Inf, n.p, \n",
    "                             s.window = 10, alg.option = \"StructTS\"){\n",
    "    data.decomp = stlplus::stlplus(data,n.p = n.p, s.window = s.window)\n",
    "    data.new = rep(0,length.out = length(data))\n",
    "    for(part in c(\"seasonal\", \"trend\", \"remainder\")){\n",
    "        data.new = data.new + na_kalman(data.decomp$data[,part],\n",
    "                                                maxgap=maxgap,\n",
    "                                                model = alg.option,\n",
    "                                        smooth = TRUE)\n",
    "    }\n",
    "    return(data.new)\n",
    "}\n",
    "\n",
    "find.na.index.length = function(x){ # antar at x er bool vektor\n",
    "    i = 1 # starting index\n",
    "    na.data = data.frame()\n",
    "    while(i <= length(x)){\n",
    "        sample.data = x[i:length(x)]\n",
    "        first = match(T, sample.data, nomatch = -1)\n",
    "        if(first < 0) {\n",
    "            break\n",
    "        }\n",
    "        last = match(F, sample.data[first:length(sample.data)], nomatch = length(sample.data[first:length(sample.data)])+1) - 2 + first\n",
    "\n",
    "        na.data = rbind(na.data, data.frame(Length = c(last-first + 1), First = c(first+i-1), Last = c(last+i-1)))\n",
    "        i = i + last\n",
    "    }\n",
    "    return(na.data)\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd377afa-b85e-4b7f-9660-0ac16171ffb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Navn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alvdal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Apelsvoll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balestrand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Bjørkelangen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Brunlanes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Øsaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Årnes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ås</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Åsbakken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Åsnes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Navn\n",
       "ID                \n",
       "10          Alvdal\n",
       "11       Apelsvoll\n",
       "12      Balestrand\n",
       "145   Bjørkelangen\n",
       "143      Brunlanes\n",
       "..             ...\n",
       "118         Øsaker\n",
       "53           Årnes\n",
       "5               Ås\n",
       "61        Åsbakken\n",
       "72           Åsnes\n",
       "\n",
       "[84 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613cd50-8242-45f1-abd3-4eccfcb6b054",
   "metadata": {},
   "source": [
    "The data is splitted among two collections of data, one is a pdf and the other is a `.xlsx` format. We start by collecting the data from the hourly data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86033f4a-3c0a-40ed-acdb-9c122b15ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_frost():\n",
    "    \"\"\"\n",
    "        Frost API deliver a json format.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def time_nibio():\n",
    "    global nibio_id #! need to change later.\n",
    "\n",
    "    # flattening dict\n",
    "\n",
    "    nibio_id_flat = []\n",
    "\n",
    "    for vals in nibio_id.values():\n",
    "        nibio_id_flat += vals\n",
    "    \n",
    "    daily_2014_2020 = {\n",
    "        y :\n",
    "            {\n",
    "                i : pd.DataFrame()  for i in range(2014,2020+1) \n",
    "            }  for y in nibio_id_flat\n",
    "    }\n",
    "    for id in nibio_id_flat:\n",
    "        for year in range(2014,2020+1):\n",
    "            daily_2014_2020[id][year] = pd.read_csv(DATA_COLLECTION_NIBIO + \"weather_data_hour_stID{}_y{}.csv\".format(id,year),\n",
    "                                              delimiter = \",\",\n",
    "                                              header = 0,\n",
    "                                              parse_dates = True,\n",
    "                                                   names = [\"Time\", \"TM\",\"RR\",\"TJM10\",\"TJM20\"])\n",
    "            daily_2014_2020[id][year][\"Time\"] = pd.to_datetime(daily_2014_2020[id][year][\"Time\"] + \"00\",\n",
    "                                                               format = \"%Y-%m-%d %H:%M:%S%z\",utc=True)\n",
    "            daily_2014_2020[id][year].set_index(\"Time\",inplace=True)\n",
    "            \n",
    "    return daily_2014_2020\n",
    "        \n",
    "\n",
    "def time_nmbu():\n",
    "    hourly_data_2013_2023 = {\n",
    "        i : pd.DataFrame() for i in range(2013,2024)\n",
    "    }\n",
    "    for i in range(2016,2024): # starts from 2016 rather than 2013 due to difference in data quality\n",
    "        raw_data = pd.read_excel(DATA_COLLECTION_TIME + \"Time{}.xlsx\".format(i)) # fetching data\n",
    "        raw_data[\"DATO\"] = pd.to_datetime(raw_data[\"DATO\"], format=\"%d%m%y %H\")\n",
    "        raw_data = raw_data.set_index(\"DATO\") # setting the date as index\n",
    "        if len(set([\"JT10\",\"JT20\",\"LT\",\"NB\"]).intersection(raw_data.columns)) == 4:\n",
    "            hourly_data_2013_2023[i] = raw_data[[\"JT10\",\"JT20\",\"LT\",\"NB\"]] # selects only a few columns.\n",
    "        else:\n",
    "            print(\"WARNING: Table:\" + str(i) + \" has missing columns.\\nMissing:\" + str([ a for a in [\"JT10\",\"JT20\",\"LT\",\"NB\"] if a not in raw_data.columns]))\n",
    "    return hourly_data_2013_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512af51-3a66-4e16-9c2d-a15eb60b83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = time_nibio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b772a661-caf2-4d9a-b402-219efd4b2044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df[22][2016].loc[:,\"TJM10\"].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa1aac-7b1a-4fd9-bf59-9719aca1c12a",
   "metadata": {},
   "source": [
    "## Linear regression function \n",
    "\n",
    "This function does a transformation of the $m\\times n$ matrix (our dataframe) to a $m \\times p$ matrix. This can be seen as a kernel trick where we transform the data to a more seperable state to improve prediction. The scema for this model is\n",
    "$$\n",
    "    (\\vec{F}\\circ \\mathbf{A})\\vec{\\beta}=\\vec{y}+\\vec{\\varepsilon})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d829d9a-6b51-4d5e-b95d-eb6397050a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_plauborg(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "        Fxn is based on a full year while df could have any range.\n",
    "    \"\"\"\n",
    "\n",
    "    data_ret = pd.DataFrame({\"B0\":df.loc[:,\"TM\"].values},columns = [\"B\"+str(i) for i in range(15)]+ [\"FS\" + str(i) for i in range(1,15)] + [\"FC\" + str(i) for i in range(1,15)])\n",
    "\n",
    "    for i in range(1,15): # 1,2\n",
    "        data_ret.loc[:,\"B\"+str(i)] = df.TM.shift(i).values\n",
    "        data_ret.loc[:,\"FS\"+str(i)] = np.sin(2*np.pi/(365*24) * ( df.index.day*24 + df.index.hour) * i)\n",
    "        data_ret.loc[:,\"FC\"+str(i)] = np.cos(2*np.pi/(365*24) * ( df.index.day*24 + df.index.hour ) * i)\n",
    "\n",
    "    return data_ret\n",
    "\n",
    "def concat_all(df: dict):\n",
    "    global_data = []\n",
    "    for key1 in df:\n",
    "        for key2 in df[key1]:\n",
    "            for d in df[key1][key2].reset_index().to_numpy().tolist():\n",
    "                global_data.append(d)\n",
    "\n",
    "    global_data =  pd.DataFrame(global_data, columns = [\"Time\",\"TM\",\"RR\",\"TJM10\",\"TJM20\"])\n",
    "    global_data[\"Time\"] = pd.to_datetime(global_data[\"Time\"])\n",
    "    global_data.set_index(\"Time\",inplace=True)\n",
    "    return global_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fafadcf-c248-49c9-9aa0-afc438a10391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TM</th>\n",
       "      <th>RR</th>\n",
       "      <th>TJM10</th>\n",
       "      <th>TJM20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-02-28 23:00:00+00:00</th>\n",
       "      <td>-9.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01 00:00:00+00:00</th>\n",
       "      <td>-10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01 01:00:00+00:00</th>\n",
       "      <td>-11.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01 02:00:00+00:00</th>\n",
       "      <td>-11.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01 03:00:00+00:00</th>\n",
       "      <td>-11.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             TM  RR  TJM10  TJM20\n",
       "Time                                             \n",
       "2018-02-28 23:00:00+00:00  -9.4 NaN   -0.1    0.4\n",
       "2018-03-01 00:00:00+00:00 -10.0 NaN   -0.1    0.4\n",
       "2018-03-01 01:00:00+00:00 -11.1 NaN   -0.1    0.4\n",
       "2018-03-01 02:00:00+00:00 -11.6 NaN   -0.1    0.4\n",
       "2018-03-01 03:00:00+00:00 -11.2 NaN   -0.1    0.4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = time_nibio()\n",
    "df[22][2018].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "760607df-e3d2-47f5-826a-f88575729936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "region_score = dict()\n",
    "\n",
    "#global_plaugborg = LinearRegression(n_jobs = -1)\n",
    "#global_default = LinearRegression(n_jobs = -1)\n",
    "\n",
    "all_data_daily = concat_all(df).dropna().resample(\"D\").mean()\n",
    "all_data = concat_all(df).dropna()#.resample(\"D\").mean()\n",
    "\n",
    "for area in nibio_id:\n",
    "    score_list_plaugborg = []\n",
    "    score_list_default = []\n",
    "    plaugborg_model = LinearRegression(n_jobs = -1)\n",
    "    default_model = LinearRegression(n_jobs = -1)\n",
    "    for id in nibio_id[area]:\n",
    "        for year in df[id]:\n",
    "            mean_df = df[id][year]#.resample(\"D\").mean()\n",
    "            data_no_na = mean_df.dropna()\n",
    "\n",
    "            if len(data_no_na) < 2:\n",
    "                continue\n",
    "\n",
    "            plaugborg_data = F_plauborg(data_no_na).dropna()\n",
    "\n",
    "            # X_train, X_test, y_train, y_test = train_test_split(plauborg_data,df[22][2016].loc[:,\"TJM10\"])\n",
    "            try:\n",
    "                scores_plaugborg = cross_validate(plaugborg_model, plaugborg_data, data_no_na.TJM20.iloc[data_no_na.shape[0]-plaugborg_data.shape[0]:],\n",
    "                                        return_estimator=True,\n",
    "                                        scoring = \"neg_root_mean_squared_error\",\n",
    "                                        cv = 6)\n",
    "                scores_default = cross_validate(default_model, data_no_na.iloc[data_no_na.shape[0]-plaugborg_data.shape[0]:,[0,1]], data_no_na.TJM20.iloc[data_no_na.shape[0]-plaugborg_data.shape[0]:],\n",
    "                                        return_estimator=True,\n",
    "                                        scoring = \"neg_root_mean_squared_error\",\n",
    "                                        cv = 6)\n",
    "\n",
    "                #scores_plaugborg_g = cross_validate(global_plaugborg, plaugborg_data, data_no_na.TJM20.iloc[50:],\n",
    "                #                        return_estimator=True,\n",
    "                #                        scoring = \"neg_root_mean_squared_error\",\n",
    "                #                        cv = 6)\n",
    "                #scores_default_g = cross_validate(global_default, data_no_na.iloc[50:,[0,1]], data_no_na.TJM20.iloc[50:],\n",
    "                #                        return_estimator=True,\n",
    "                #                        scoring = \"neg_root_mean_squared_error\",\n",
    "                #                        cv = 6)\n",
    "            except ValueError as msg:\n",
    "                print(\"something wrong: id:\" + str(id) + \" year:\" + str(year))\n",
    "                raise ValueError(msg)\n",
    "            score_list_plaugborg.append(scores_plaugborg)\n",
    "            score_list_default.append(scores_default)\n",
    "            \n",
    "            max_score_p = np.argmax(scores_plaugborg[\"test_score\"]) # finding the best\n",
    "            max_score_d = np.argmax(scores_default[\"test_score\"]) # finding the best\n",
    "\n",
    "            #max_score_pg = np.argmax(scores_plaugborg_g[\"test_score\"]) # finding the best\n",
    "            #max_score_dg = np.argmax(scores_default_g[\"test_score\"]) # finding the best\n",
    "            \n",
    "            plauborg_model = scores_plaugborg[\"estimator\"][max_score_p].fit(plaugborg_data,data_no_na.TJM20.iloc[data_no_na.shape[0]-plaugborg_data.shape[0]:])\n",
    "            default_model = scores_default[\"estimator\"][max_score_d].fit(data_no_na.iloc[data_no_na.shape[0]-plaugborg_data.shape[0]:,[0,1]],data_no_na.TJM20.iloc[data_no_na.shape[0]-plaugborg_data.shape[0]:])\n",
    "\n",
    "            #global_plaugborg = scores_plaugborg_g[\"estimator\"][max_score_pg].fit(plaugborg_data,data_no_na.TJM20.iloc[50:])\n",
    "            #global_default = scores_plaugborg_g[\"estimator\"][max_score_dg].fit(data_no_na.iloc[50:,[0,1]],data_no_na.TJM20.iloc[50:])\n",
    "    region_score[area] = {\"default\":np.mean(scores_plaugborg[\"test_score\"]), \"plaugborg\":np.mean(scores_default[\"test_score\"])}\n",
    "\n",
    "#gd = cross_validate(global_default, all_data.iloc[50:,[0,1]], all_data.TJM20.iloc[50:],\n",
    "#                                        scoring = \"neg_root_mean_squared_error\",\n",
    "#                                        cv = 6)\n",
    "#gp = cross_validate(global_plaugborg, F_plauborg(all_data_daily).iloc[50:], all_data_daily.TJM20.iloc[50:],\n",
    "#                                        scoring = \"neg_root_mean_squared_error\",\n",
    "#                                        cv = 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "199645d6-3a51-44a8-85d4-b983a2b5517b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Innlandet': {'default': -2.862767814711662, 'plaugborg': -3.864984081746504},\n",
       " 'Trøndelag': {'default': -3.0363603245803024,\n",
       "  'plaugborg': -3.1560967715828974},\n",
       " 'Østfold': {'default': -2.7548736081575043, 'plaugborg': -3.3597767175319704},\n",
       " 'Vestfold': {'default': -2.5684486917392086,\n",
       "  'plaugborg': -3.0619015186514003}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92a1afb7-ed45-4d79-a912-c4b3f6266870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2012-06-30', '2012-07-01', '2012-07-02', '2012-07-03',\n",
       "               '2012-07-04', '2012-07-05', '2012-07-06', '2012-07-07',\n",
       "               '2012-07-08', '2012-07-09'],\n",
       "              dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "dti = pd.date_range(\n",
    "    start=\"7/1/2012\", end=\"7/10/2012\"\n",
    ")\n",
    "dti\n",
    "dti.shift(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecfd404e-51f1-40ff-ab1d-623fb8e2b80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2018-02-28 23:00:00+00:00', '2018-03-01 00:00:00+00:00',\n",
      "               '2018-03-01 01:00:00+00:00', '2018-03-01 02:00:00+00:00',\n",
      "               '2018-03-01 03:00:00+00:00', '2018-03-01 04:00:00+00:00',\n",
      "               '2018-03-01 05:00:00+00:00', '2018-03-01 06:00:00+00:00',\n",
      "               '2018-03-01 07:00:00+00:00', '2018-03-01 08:00:00+00:00',\n",
      "               ...\n",
      "               '2018-10-31 13:00:00+00:00', '2018-10-31 14:00:00+00:00',\n",
      "               '2018-10-31 15:00:00+00:00', '2018-10-31 16:00:00+00:00',\n",
      "               '2018-10-31 17:00:00+00:00', '2018-10-31 18:00:00+00:00',\n",
      "               '2018-10-31 19:00:00+00:00', '2018-10-31 20:00:00+00:00',\n",
      "               '2018-10-31 21:00:00+00:00', '2018-10-31 22:00:00+00:00'],\n",
      "              dtype='datetime64[ns, UTC]', name='Time', length=5880, freq=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448,\n",
       "       ...\n",
       "       7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318],\n",
       "      dtype='int32', name='Time', length=5880)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[22][2018].index)\n",
    "df[22][2018].index.day_of_year*24 + df[22][2018].index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32135863-02f3-4d62-bb68-98dadef418d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
